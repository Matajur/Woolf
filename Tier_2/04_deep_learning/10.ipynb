{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2. Module 4 - Deep Learning. Homework\n",
    "\n",
    "## Lessons 9-10: Introduction to NLP. Word Embeddinges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to create a model that can distinguish spam from legitimate messages using text data techniques. The deep learning model must classify emails into spam (unwanted messages) and \"ham\" (legitimate messages) based on the provided [Email Spam Detection Dataset](https://www.kaggle.com/datasets/shantanudhakadd/email-spam-detection-dataset-classification), which contains samples of emails classified as spam or ham.\n",
    "\n",
    "Technical task:\n",
    "1. Conduct a preliminary analysis of the data.\n",
    "2. Prepare the data for training the model.\n",
    "3. Apply count-based methods and use pre-trained embeddings to represent the text data and build a classification model.\n",
    "4. Evaluate the performance of the model and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:40.030007Z",
     "iopub.status.busy": "2024-12-07T19:31:40.028119Z",
     "iopub.status.idle": "2024-12-07T19:31:40.040094Z",
     "shell.execute_reply": "2024-12-07T19:31:40.038649Z",
     "shell.execute_reply.started": "2024-12-07T19:31:40.029941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from gensim.models import KeyedVectors #  implements word vectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import spacy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:43.806023Z",
     "iopub.status.busy": "2024-12-07T19:31:43.805459Z",
     "iopub.status.idle": "2024-12-07T19:31:43.812799Z",
     "shell.execute_reply": "2024-12-07T19:31:43.811240Z",
     "shell.execute_reply.started": "2024-12-07T19:31:43.805974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"/kaggle/input/email-spam-detection-dataset-classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:20.090161Z",
     "iopub.status.busy": "2024-12-07T19:31:20.089823Z",
     "iopub.status.idle": "2024-12-07T19:31:21.307067Z",
     "shell.execute_reply": "2024-12-07T19:31:21.305432Z",
     "shell.execute_reply.started": "2024-12-07T19:31:20.090128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/kaggle/input/email-spam-detection-dataset-classification/\u001b[00m\n",
      "`-- spam.csv\n",
      "\n",
      "0 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree {dataset_dir} -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:47.779281Z",
     "iopub.status.busy": "2024-12-07T19:31:47.778857Z",
     "iopub.status.idle": "2024-12-07T19:31:47.835862Z",
     "shell.execute_reply": "2024-12-07T19:31:47.834577Z",
     "shell.execute_reply.started": "2024-12-07T19:31:47.779243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir + \"spam.csv\", encoding = \"ISO-8859-1\") \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:50.679517Z",
     "iopub.status.busy": "2024-12-07T19:31:50.678187Z",
     "iopub.status.idle": "2024-12-07T19:31:50.714009Z",
     "shell.execute_reply": "2024-12-07T19:31:50.712553Z",
     "shell.execute_reply.started": "2024-12-07T19:31:50.679459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:53.722133Z",
     "iopub.status.busy": "2024-12-07T19:31:53.721686Z",
     "iopub.status.idle": "2024-12-07T19:31:53.747190Z",
     "shell.execute_reply": "2024-12-07T19:31:53.744536Z",
     "shell.execute_reply.started": "2024-12-07T19:31:53.722093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\Wen u miss someone</td>\n",
       "      <td>the person is definitely special for u..... B...</td>\n",
       "      <td>why to miss them</td>\n",
       "      <td>just Keep-in-touch\\\" gdeve..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>ham</td>\n",
       "      <td>Edison has rightly said, \\A fool can ask more ...</td>\n",
       "      <td>GN</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just lov this line: \\Hurt me with the truth</td>\n",
       "      <td>I don't mind</td>\n",
       "      <td>i wil tolerat.bcs ur my someone..... But</td>\n",
       "      <td>Never comfort me with a lie\\\" gud ni8 and swe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DE...</td>\n",
       "      <td>HAD A COOL NYTHO</td>\n",
       "      <td>TX 4 FONIN HON</td>\n",
       "      <td>CALL 2MWEN IM BK FRMCLOUD 9! J X\\\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>ham</td>\n",
       "      <td>When I was born, GOD said, \\Oh No! Another IDI...</td>\n",
       "      <td>GOD said</td>\n",
       "      <td>\\\"OH No! COMPETITION\\\". Who knew</td>\n",
       "      <td>one day these two will become FREINDS FOREVER!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1                                                 v2  \\\n",
       "281   ham                                \\Wen u miss someone   \n",
       "1038  ham  Edison has rightly said, \\A fool can ask more ...   \n",
       "2255  ham      I just lov this line: \\Hurt me with the truth   \n",
       "3525  ham  \\HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DE...   \n",
       "4668  ham  When I was born, GOD said, \\Oh No! Another IDI...   \n",
       "\n",
       "                                             Unnamed: 2  \\\n",
       "281    the person is definitely special for u..... B...   \n",
       "1038                                                 GN   \n",
       "2255                                       I don't mind   \n",
       "3525                                   HAD A COOL NYTHO   \n",
       "4668                                           GOD said   \n",
       "\n",
       "                                    Unnamed: 3  \\\n",
       "281                           why to miss them   \n",
       "1038                                        GE   \n",
       "2255  i wil tolerat.bcs ur my someone..... But   \n",
       "3525                            TX 4 FONIN HON   \n",
       "4668          \\\"OH No! COMPETITION\\\". Who knew   \n",
       "\n",
       "                                             Unnamed: 4  \n",
       "281                       just Keep-in-touch\\\" gdeve..\"  \n",
       "1038                                            GNT:-)\"  \n",
       "2255   Never comfort me with a lie\\\" gud ni8 and swe...  \n",
       "3525                CALL 2MWEN IM BK FRMCLOUD 9! J X\\\"\"  \n",
       "4668    one day these two will become FREINDS FOREVER!\"  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_nan = df.dropna(subset=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\n",
    "df_no_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:56.378253Z",
     "iopub.status.busy": "2024-12-07T19:31:56.377824Z",
     "iopub.status.idle": "2024-12-07T19:31:56.387935Z",
     "shell.execute_reply": "2024-12-07T19:31:56.386660Z",
     "shell.execute_reply.started": "2024-12-07T19:31:56.378215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = df[\"v1\"].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous data analysis, we can conclude that the dataset is very unbalanced, with the number of ham messages significantly exceeding the number of spam.\n",
    "\n",
    "In addition, columns 'v3' - 'v5' basically contain only blanks, and the available values ​​mostly belong to ham messages. Decision is to delete the last three columns, because they carry almost no information.\n",
    "\n",
    "Also, let's rename columns 'v1' and 'v2' for better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:31:58.619057Z",
     "iopub.status.busy": "2024-12-07T19:31:58.618103Z",
     "iopub.status.idle": "2024-12-07T19:31:58.633918Z",
     "shell.execute_reply": "2024-12-07T19:31:58.632413Z",
     "shell.execute_reply.started": "2024-12-07T19:31:58.618991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep = df[[\"v1\", \"v2\"]].rename({\"v1\": \"label\", \"v2\": \"message\"}, axis=1)\n",
    "\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's frop duplicates, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:01.604960Z",
     "iopub.status.busy": "2024-12-07T19:32:01.604544Z",
     "iopub.status.idle": "2024-12-07T19:32:01.619890Z",
     "shell.execute_reply": "2024-12-07T19:32:01.618620Z",
     "shell.execute_reply.started": "2024-12-07T19:32:01.604926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4516\n",
       "spam     653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep = df_prep.drop_duplicates().reset_index(drop=True)\n",
    "label_counts = df_prep[\"label\"].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the target variable into 0 and 1, typical labels for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:03.920732Z",
     "iopub.status.busy": "2024-12-07T19:32:03.920245Z",
     "iopub.status.idle": "2024-12-07T19:32:03.938178Z",
     "shell.execute_reply": "2024-12-07T19:32:03.936962Z",
     "shell.execute_reply.started": "2024-12-07T19:32:03.920688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep[\"label\"] = df_prep[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions from [source](http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:12.129399Z",
     "iopub.status.busy": "2024-12-07T19:32:12.128872Z",
     "iopub.status.idle": "2024-12-07T19:32:12.143491Z",
     "shell.execute_reply": "2024-12-07T19:32:12.141964Z",
     "shell.execute_reply.started": "2024-12-07T19:32:12.129345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:16.380010Z",
     "iopub.status.busy": "2024-12-07T19:32:16.378360Z",
     "iopub.status.idle": "2024-12-07T19:32:16.394624Z",
     "shell.execute_reply": "2024-12-07T19:32:16.393115Z",
     "shell.execute_reply.started": "2024-12-07T19:32:16.379960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({'also', 'would', 'much', 'many'})\n",
    "\n",
    "negations = {\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'shan',\n",
    "    \"shan't\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\"\n",
    "}\n",
    "\n",
    "stop_words = stop_words.difference(negations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:20.375569Z",
     "iopub.status.busy": "2024-12-07T19:32:20.375149Z",
     "iopub.status.idle": "2024-12-07T19:32:21.405905Z",
     "shell.execute_reply": "2024-12-07T19:32:21.404363Z",
     "shell.execute_reply.started": "2024-12-07T19:32:20.375530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # During text normalization below, lemmatization is used instead of stemming\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:24.360269Z",
     "iopub.status.busy": "2024-12-07T19:32:24.359400Z",
     "iopub.status.idle": "2024-12-07T19:32:24.368209Z",
     "shell.execute_reply": "2024-12-07T19:32:24.366904Z",
     "shell.execute_reply.started": "2024-12-07T19:32:24.360229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(raw_review):\n",
    "    \n",
    "    # Remove html tags\n",
    "    text = re.sub(\"<[^>]*>\", \" \", raw_review) # match <> and everything in between. [^>] - match everything except >\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(\"\\\\S*@\\\\S*[\\\\s]+\", \" \", text) # match non-whitespace characters, @ and a whitespaces in the end\n",
    "    \n",
    "    # remove links\n",
    "    text = re.sub(\"https?:\\\\/\\\\/.*?[\\\\s]+\", \" \", text) # match http, s - zero or once, //, \n",
    "                                                    # any char 0-unlimited, whitespaces in the end\n",
    "        \n",
    "     # Convert to lower case, split into individual words\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Replace contractions with their full versions\n",
    "    text = [contractions.get(word) if word in contractions else word \n",
    "            for word in text]\n",
    "   \n",
    "    # Re-splitting for the correct stop-words extraction\n",
    "    text = \" \".join(text).split()    \n",
    "    \n",
    "    # Remove stop words\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Remove non-letters        \n",
    "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text) # match everything except letters and '\n",
    "\n",
    "    # Stem words. Need to define porter stemmer above\n",
    "    # text = [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "    # Lemmatize words. Need to define lemmatizer above\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc if len(token.lemma_) > 1 ])\n",
    "    \n",
    "    # Remove excesive whitespaces\n",
    "    text = re.sub(\"[\\\\s]+\", \" \", text)    \n",
    "    \n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:27.472556Z",
     "iopub.status.busy": "2024-12-07T19:32:27.472119Z",
     "iopub.status.idle": "2024-12-07T19:32:46.113811Z",
     "shell.execute_reply": "2024-12-07T19:32:46.112463Z",
     "shell.execute_reply.started": "2024-12-07T19:32:27.472475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aced3149a2f47d5a584f8038a52abf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah not think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     text_normalized  \n",
       "0  go jurong point crazy available bugis great wo...  \n",
       "1                              ok lar joking wif oni  \n",
       "2  free entry wkly comp win fa cup final tkts st ...  \n",
       "3                      dun say early hor already say  \n",
       "4            nah not think go usf life around though  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep['text_normalized'] = df_prep['message'].progress_apply(normalize_text)\n",
    "\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test / train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:49.181886Z",
     "iopub.status.busy": "2024-12-07T19:32:49.181504Z",
     "iopub.status.idle": "2024-12-07T19:32:49.196164Z",
     "shell.execute_reply": "2024-12-07T19:32:49.194628Z",
     "shell.execute_reply.started": "2024-12-07T19:32:49.181856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_idxs = df_prep.sample(frac=0.8, random_state=42).index\n",
    "test_idxs = [idx for idx in df_prep.index if idx not in train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:32:52.142981Z",
     "iopub.status.busy": "2024-12-07T19:32:52.142547Z",
     "iopub.status.idle": "2024-12-07T19:32:52.152969Z",
     "shell.execute_reply": "2024-12-07T19:32:52.151885Z",
     "shell.execute_reply.started": "2024-12-07T19:32:52.142943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = df_prep.loc[train_idxs, 'text_normalized']\n",
    "X_test = df_prep.loc[test_idxs, 'text_normalized']\n",
    "\n",
    "y_train = df_prep.loc[train_idxs, 'label']\n",
    "y_test = df_prep.loc[test_idxs, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of BoW and TF-IDF methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training two different vectorizer objects which use n-grams and all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:45.442957Z",
     "iopub.status.busy": "2024-12-07T19:33:45.442266Z",
     "iopub.status.idle": "2024-12-07T19:33:45.448779Z",
     "shell.execute_reply": "2024-12-07T19:33:45.447540Z",
     "shell.execute_reply.started": "2024-12-07T19:33:45.442910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ngrams=(1,2) # using unigrams and bigrams\n",
    "max_feats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:48.044987Z",
     "iopub.status.busy": "2024-12-07T19:33:48.044594Z",
     "iopub.status.idle": "2024-12-07T19:33:48.214344Z",
     "shell.execute_reply": "2024-12-07T19:33:48.213033Z",
     "shell.execute_reply.started": "2024-12-07T19:33:48.044953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30989"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_cv = CountVectorizer(ngram_range=ngrams, max_features=max_feats).fit(X_train)\n",
    "\n",
    "len(vect_cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency-inverse document frequency vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:51.032030Z",
     "iopub.status.busy": "2024-12-07T19:33:51.031626Z",
     "iopub.status.idle": "2024-12-07T19:33:51.202690Z",
     "shell.execute_reply": "2024-12-07T19:33:51.200797Z",
     "shell.execute_reply.started": "2024-12-07T19:33:51.031994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30989"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_tfidf = TfidfVectorizer(ngram_range=ngrams, max_features=max_feats).fit(X_train)\n",
    "\n",
    "len(vect_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:54.136938Z",
     "iopub.status.busy": "2024-12-07T19:33:54.136507Z",
     "iopub.status.idle": "2024-12-07T19:33:54.174485Z",
     "shell.execute_reply": "2024-12-07T19:33:54.173339Z",
     "shell.execute_reply.started": "2024-12-07T19:33:54.136901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aah', 'aah cuddle', 'aah speak', 'aaooooright',\n",
       "       'aaooooright work'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_cv.get_feature_names_out()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:57.016039Z",
     "iopub.status.busy": "2024-12-07T19:33:57.015635Z",
     "iopub.status.idle": "2024-12-07T19:33:57.054111Z",
     "shell.execute_reply": "2024-12-07T19:33:57.052866Z",
     "shell.execute_reply.started": "2024-12-07T19:33:57.016004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aah', 'aah cuddle', 'aah speak', 'aaooooright',\n",
       "       'aaooooright work'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_tfidf.get_feature_names_out()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the documents in the training and testing data to a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:33:59.662308Z",
     "iopub.status.busy": "2024-12-07T19:33:59.661896Z",
     "iopub.status.idle": "2024-12-07T19:33:59.773372Z",
     "shell.execute_reply": "2024-12-07T19:33:59.772008Z",
     "shell.execute_reply.started": "2024-12-07T19:33:59.662272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_vectorized_cv = vect_cv.transform(X_train)\n",
    "X_test_vectorized_cv = vect_cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:34:02.766890Z",
     "iopub.status.busy": "2024-12-07T19:34:02.765991Z",
     "iopub.status.idle": "2024-12-07T19:34:02.880538Z",
     "shell.execute_reply": "2024-12-07T19:34:02.879263Z",
     "shell.execute_reply.started": "2024-12-07T19:34:02.766848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_vectorized_tfidf = vect_tfidf.transform(X_train)\n",
    "X_test_vectorized_tfidf = vect_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-trained embedding FastText will be used to accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained on WikiNews embeddings from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:51:32.636744Z",
     "iopub.status.busy": "2024-12-07T19:51:32.636314Z",
     "iopub.status.idle": "2024-12-07T19:51:32.642502Z",
     "shell.execute_reply": "2024-12-07T19:51:32.641065Z",
     "shell.execute_reply.started": "2024-12-07T19:51:32.636712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fasttext_path = '/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:51:36.318313Z",
     "iopub.status.busy": "2024-12-07T19:51:36.317901Z",
     "iopub.status.idle": "2024-12-07T19:51:36.324557Z",
     "shell.execute_reply": "2024-12-07T19:51:36.323260Z",
     "shell.execute_reply.started": "2024-12-07T19:51:36.318276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:51:40.128823Z",
     "iopub.status.busy": "2024-12-07T19:51:40.128414Z",
     "iopub.status.idle": "2024-12-07T19:52:58.313813Z",
     "shell.execute_reply": "2024-12-07T19:52:58.312556Z",
     "shell.execute_reply.started": "2024-12-07T19:51:40.128790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fasttext_embeddings = load_embeddings(fasttext_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for converting text data into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:58:07.403004Z",
     "iopub.status.busy": "2024-12-07T19:58:07.402523Z",
     "iopub.status.idle": "2024-12-07T19:58:07.410050Z",
     "shell.execute_reply": "2024-12-07T19:58:07.408637Z",
     "shell.execute_reply.started": "2024-12-07T19:58:07.402936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_to_embedding(text, embeddings_index, embedding_dim=300):\n",
    "    words = text.split()  # Tokenize text into words\n",
    "    embeddings = [embeddings_index.get(word, np.zeros(embedding_dim)) for word in words]\n",
    "    if embeddings:  # If the text has valid words\n",
    "        return np.mean(embeddings, axis=0)  # Average word embeddings\n",
    "    else:  # If no valid words in the text\n",
    "        return np.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert train and validation datasets to embaddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:59:02.476503Z",
     "iopub.status.busy": "2024-12-07T19:59:02.476031Z",
     "iopub.status.idle": "2024-12-07T19:59:02.482339Z",
     "shell.execute_reply": "2024-12-07T19:59:02.481135Z",
     "shell.execute_reply.started": "2024-12-07T19:59:02.476433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_dataset_to_embeddings(dataset, embeddings_index, embedding_dim=300):\n",
    "    return np.array([text_to_embedding(text, embeddings_index, embedding_dim) for text in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:59:33.204423Z",
     "iopub.status.busy": "2024-12-07T19:59:33.203143Z",
     "iopub.status.idle": "2024-12-07T19:59:33.431408Z",
     "shell.execute_reply": "2024-12-07T19:59:33.430221Z",
     "shell.execute_reply.started": "2024-12-07T19:59:33.204377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_embeddings = convert_dataset_to_embeddings(X_train, fasttext_embeddings)\n",
    "X_test_embeddings = convert_dataset_to_embeddings(X_test, fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T19:59:47.190907Z",
     "iopub.status.busy": "2024-12-07T19:59:47.190474Z",
     "iopub.status.idle": "2024-12-07T19:59:47.201177Z",
     "shell.execute_reply": "2024-12-07T19:59:47.199876Z",
     "shell.execute_reply.started": "2024-12-07T19:59:47.190868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.44466653e-01,  7.34333321e-02,  2.71666665e-02,  2.91000009e-02,\n",
       "        9.23333392e-02,  8.66666716e-03,  1.01333335e-02,  6.39000013e-02,\n",
       "        4.94000018e-02,  5.71333319e-02,  1.42666651e-02, -8.85666609e-02,\n",
       "        4.80666645e-02,  6.28666654e-02, -3.33332755e-05, -3.49999964e-03,\n",
       "        7.04333335e-02, -9.06000063e-02, -1.47666678e-01, -3.73000018e-02,\n",
       "       -2.38133326e-01, -9.63666663e-02,  1.23133332e-01,  3.40666659e-02,\n",
       "       -5.15666716e-02, -1.40133336e-01,  1.08333305e-02,  2.94999983e-02,\n",
       "        1.90000108e-03,  4.24666665e-02,  1.29133329e-01,  8.56333300e-02,\n",
       "        6.51333332e-02,  1.06666656e-02,  1.89666655e-02,  1.59966663e-01,\n",
       "        1.51666701e-02, -3.09666693e-02,  7.86666665e-03,  5.83333010e-03,\n",
       "       -5.61666675e-02, -6.70999959e-02, -5.96666569e-03,  1.62333325e-02,\n",
       "        3.65666673e-02, -8.49666670e-02, -2.21333355e-02,  4.26666699e-02,\n",
       "        5.13333315e-03, -5.96666569e-03, -4.26666951e-03, -6.01666681e-02,\n",
       "       -7.34700024e-01,  1.10999988e-02, -8.39666724e-02, -8.43000039e-02,\n",
       "        2.32333336e-02, -4.71666642e-02, -5.42666651e-02, -4.10666689e-02,\n",
       "        1.02366664e-01, -1.53666670e-02, -8.11999962e-02,  4.96666646e-03,\n",
       "        5.58999963e-02,  1.23333430e-03,  5.97999990e-02, -6.25999942e-02,\n",
       "        3.05333305e-02, -2.90666670e-02, -1.26466677e-01,  5.39333336e-02,\n",
       "       -4.83666696e-02,  1.29199997e-01, -1.19666671e-02, -1.96799994e-01,\n",
       "        2.46666688e-02,  2.47333348e-02,  7.33333305e-02, -2.35666689e-02,\n",
       "        8.29999987e-03, -7.19333291e-02,  1.83666665e-02, -1.57399997e-01,\n",
       "       -2.48333309e-02,  8.66000056e-02, -3.18333358e-02,  1.00666685e-02,\n",
       "        8.48000050e-02,  8.29999968e-02,  9.79999732e-03, -3.07000037e-02,\n",
       "       -1.91566661e-01, -8.26999992e-02,  8.30999985e-02,  5.75000010e-02,\n",
       "        1.31999999e-01,  2.96000000e-02,  8.38333368e-02, -3.96333374e-02,\n",
       "       -9.43000019e-02,  1.28999993e-01,  5.59999980e-02, -9.40000042e-02,\n",
       "       -1.56666655e-02, -8.73333216e-03, -2.52999980e-02,  6.59000054e-02,\n",
       "       -1.21333331e-01,  4.85333316e-02,  2.56333351e-02,  1.20999962e-02,\n",
       "        8.39666724e-02, -7.06333295e-02, -1.18633330e-01,  7.25333318e-02,\n",
       "        1.18666664e-02, -1.23366676e-01, -2.17733338e-01, -2.98000008e-01,\n",
       "        2.59666648e-02,  7.20333382e-02, -1.85733318e-01,  7.16666644e-03,\n",
       "        2.13000011e-02,  5.42999990e-02,  6.65666685e-02, -4.16000001e-02,\n",
       "       -8.16999972e-02, -1.19066663e-01,  1.04000010e-02, -5.28333336e-02,\n",
       "        1.37999998e-02,  1.45600006e-01,  1.33000016e-02,  6.18999936e-02,\n",
       "       -1.99666675e-02,  3.68666612e-02,  6.46333322e-02,  7.39666671e-02,\n",
       "        9.46666673e-03, -3.69999968e-02, -1.28566667e-01,  9.46666673e-02,\n",
       "        2.54999995e-02, -6.34333342e-02,  1.51999937e-02,  1.02699995e-01,\n",
       "       -4.63333316e-02,  1.46666600e-03, -6.83333352e-02,  2.76333336e-02,\n",
       "        8.28000009e-02,  5.78333326e-02,  6.34333342e-02,  3.33666652e-02,\n",
       "       -6.28999993e-02,  3.62333357e-02,  2.72666663e-02,  6.33334101e-04,\n",
       "        4.33666669e-02, -2.28666663e-02, -3.24333347e-02,  6.35333285e-02,\n",
       "        1.91333350e-02,  5.85666709e-02,  9.49666724e-02,  3.75666656e-02,\n",
       "       -5.29999994e-02, -1.74166679e-01,  2.38333289e-02, -2.39666700e-02,\n",
       "       -1.18666664e-02,  6.31000027e-02, -6.26333356e-02,  9.00666714e-02,\n",
       "        2.14699984e-01, -6.61999956e-02,  3.11333332e-02,  2.77333334e-02,\n",
       "        7.70000136e-03,  4.23333282e-03, -4.90000052e-03,  2.16666665e-02,\n",
       "        9.93666649e-02, -2.64633328e-01,  5.69666661e-02, -1.66667000e-04,\n",
       "       -6.78333342e-02, -8.93333461e-03,  6.01666719e-02,  8.00000057e-02,\n",
       "        2.79000010e-02, -9.16999951e-02, -2.06333324e-02, -1.53333449e-03,\n",
       "       -9.90333334e-02,  9.57666636e-02,  5.53333350e-02, -1.46666483e-03,\n",
       "        3.92999984e-02,  3.06666642e-03, -5.60000055e-02, -8.11333358e-02,\n",
       "       -2.10000086e-03,  5.87666668e-02, -1.05366670e-01,  7.14999959e-02,\n",
       "       -2.66000032e-02,  4.83999997e-02,  1.09299995e-01, -2.55333334e-02,\n",
       "        4.67333309e-02,  4.11333293e-02, -9.62333307e-02,  8.79999716e-03,\n",
       "        7.84666687e-02,  2.98999976e-02, -1.11766666e-01,  1.82333335e-01,\n",
       "       -3.29999998e-02, -5.55999987e-02,  3.61000039e-02,  1.40366673e-01,\n",
       "       -4.17333357e-02, -1.74766660e-01,  2.06999984e-02, -1.00966670e-01,\n",
       "        1.64000005e-01, -1.11566670e-01,  4.36666608e-03, -1.58333331e-02,\n",
       "        1.98866665e-01,  1.87000018e-02, -1.76333357e-02, -1.01866663e-01,\n",
       "       -1.51700005e-01, -6.36333302e-02, -1.67400002e-01,  2.29900002e-01,\n",
       "       -6.59666657e-02, -8.25666711e-02,  1.05999978e-02, -4.94000018e-02,\n",
       "       -5.91333322e-02, -1.71333347e-02, -2.56666671e-02, -1.01099998e-01,\n",
       "        4.45999987e-02,  3.34266663e-01, -9.37666669e-02, -1.20799996e-01,\n",
       "       -1.61233321e-01, -1.39200002e-01, -2.14333329e-02, -1.47333341e-02,\n",
       "       -4.36666645e-02,  6.20999970e-02, -3.22999991e-02,  6.05333336e-02,\n",
       "        1.87666696e-02, -3.49333361e-02, -1.58366665e-01, -6.14333302e-02,\n",
       "       -2.16399983e-01,  4.47999984e-02, -6.41999990e-02, -1.13233328e-01,\n",
       "       -9.93666723e-02, -3.81666720e-02,  4.09999862e-03, -2.31000017e-02,\n",
       "        4.03666683e-02, -2.06200004e-01,  4.66000028e-02,  1.05633326e-01,\n",
       "        9.06333327e-02,  1.80199996e-01,  3.24666649e-02,  5.02666645e-02,\n",
       "        3.26333344e-02, -2.01333314e-02,  6.03000037e-02,  1.81333330e-02,\n",
       "       -1.10533334e-01, -1.30799994e-01,  5.66666611e-02, -1.45666674e-02,\n",
       "       -3.82333361e-02,  3.06666642e-02,  6.75666630e-02, -4.25000004e-02,\n",
       "       -3.81000005e-02,  1.27166674e-01, -6.05333298e-02, -2.66666710e-03,\n",
       "       -1.60999987e-02,  2.85333339e-02,  1.54666677e-02, -4.21000011e-02])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building and training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model builder and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:19:47.854332Z",
     "iopub.status.busy": "2024-12-07T20:19:47.853284Z",
     "iopub.status.idle": "2024-12-07T20:19:47.859502Z",
     "shell.execute_reply": "2024-12-07T20:19:47.858176Z",
     "shell.execute_reply.started": "2024-12-07T20:19:47.854289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def get_trained_model(x_train, y_train):\n",
    "    model = GradientBoostingRegressor(random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on BoW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:19:51.069554Z",
     "iopub.status.busy": "2024-12-07T20:19:51.069140Z",
     "iopub.status.idle": "2024-12-07T20:19:54.258397Z",
     "shell.execute_reply": "2024-12-07T20:19:54.257164Z",
     "shell.execute_reply.started": "2024-12-07T20:19:51.069516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_bow = get_trained_model(X_train_vectorized_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on TF-IDF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:19:57.051398Z",
     "iopub.status.busy": "2024-12-07T20:19:57.051003Z",
     "iopub.status.idle": "2024-12-07T20:20:00.898560Z",
     "shell.execute_reply": "2024-12-07T20:20:00.897172Z",
     "shell.execute_reply.started": "2024-12-07T20:19:57.051361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_tfidf = get_trained_model(X_train_vectorized_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:20:03.896316Z",
     "iopub.status.busy": "2024-12-07T20:20:03.895944Z",
     "iopub.status.idle": "2024-12-07T20:20:50.738548Z",
     "shell.execute_reply": "2024-12-07T20:20:50.737612Z",
     "shell.execute_reply.started": "2024-12-07T20:20:03.896284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_fasttext = get_trained_model(X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the model evaluation. In our case AUC (Area Under the Curve) metric is used, as regressor used for classification predicts probabilities of belonging to a ham o spam class insted of discrete predictions, which are suitable for the standard Accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:30:38.333212Z",
     "iopub.status.busy": "2024-12-07T20:30:38.332786Z",
     "iopub.status.idle": "2024-12-07T20:30:38.339082Z",
     "shell.execute_reply": "2024-12-07T20:30:38.337843Z",
     "shell.execute_reply.started": "2024-12-07T20:30:38.333177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    predictions = model.predict(x_test)\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on BoW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:30:40.725141Z",
     "iopub.status.busy": "2024-12-07T20:30:40.724731Z",
     "iopub.status.idle": "2024-12-07T20:30:40.737911Z",
     "shell.execute_reply": "2024-12-07T20:30:40.736712Z",
     "shell.execute_reply.started": "2024-12-07T20:30:40.725105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:       0.9600751241972616\n"
     ]
    }
   ],
   "source": [
    "predictions_bow = evaluate_model(model_bow, X_test_vectorized_cv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on TF-IDF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:30:44.556380Z",
     "iopub.status.busy": "2024-12-07T20:30:44.555977Z",
     "iopub.status.idle": "2024-12-07T20:30:44.568087Z",
     "shell.execute_reply": "2024-12-07T20:30:44.566621Z",
     "shell.execute_reply.started": "2024-12-07T20:30:44.556343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:       0.9611190336381175\n"
     ]
    }
   ],
   "source": [
    "predictions_tfidf = evaluate_model(model_tfidf, X_test_vectorized_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T20:30:49.094416Z",
     "iopub.status.busy": "2024-12-07T20:30:49.094046Z",
     "iopub.status.idle": "2024-12-07T20:30:49.108705Z",
     "shell.execute_reply": "2024-12-07T20:30:49.107297Z",
     "shell.execute_reply.started": "2024-12-07T20:30:49.094371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:       0.9687572817343809\n"
     ]
    }
   ],
   "source": [
    "predictions_fasttextw = evaluate_model(model_fasttext, X_test_embeddings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Analysis and interpretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models, built on word vectorization using Bag of Words, Term frequency-inverse document frequency, and also using FastText embeddings pretrained on WikiNews, showed approximately the same results, their accuracy, calculated using AUC, was in the range of 96-97%. The model built on FastText embeddings coped with the task a little better, but with a small margin. This suggests that all three methods considered are effective and can be applied in practice.\n",
    "\n",
    "To improve results, can try balancing the spam and ham classes, as well as using a FastText embeddings trained on a larger amount of data than WikiNews."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1961542,
     "sourceId": 3235802,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9235,
     "sourceId": 12944,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
