{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1. Module 3: Basic Algorithms and Data Structures\n",
    "\n",
    "## Topic 5 - Search algorithms\n",
    "\n",
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a `delete` method to delete key-value pairs of the `HashTable` table, which is implemented in the outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['apple', 10], ['orange', 20], ['banana', 30]], [], [], [], []]\n",
      "None\n",
      "None\n",
      "None\n",
      "[[['apple', 10]], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "class HashTable:\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.table = [[] for _ in range(self.size)]\n",
    "\n",
    "    def hash_function(self, key):\n",
    "        return hash(key) % self.size\n",
    "\n",
    "    def insert(self, key, value):\n",
    "        key_hash = self.hash_function(key)\n",
    "        key_value = [key, value]\n",
    "\n",
    "        if self.table[key_hash] is None:\n",
    "            self.table[key_hash] = [key_value]\n",
    "            return True\n",
    "        else:\n",
    "            for pair in self.table[key_hash]:\n",
    "                if pair[0] == key:\n",
    "                    pair[1] = value\n",
    "                    return True\n",
    "            self.table[key_hash].append(key_value)\n",
    "            return True\n",
    "\n",
    "    def get(self, key):\n",
    "        key_hash = self.hash_function(key)\n",
    "        if self.table[key_hash] is not None:\n",
    "            for pair in self.table[key_hash]:\n",
    "                if pair[0] == key:\n",
    "                    return pair[1]\n",
    "        return None\n",
    "\n",
    "    def delete(self, key):\n",
    "        key_hash = self.hash_function(key)\n",
    "        if self.table[key_hash] is not None:\n",
    "            for index, pair in enumerate(self.table[key_hash]):\n",
    "                if pair[0] == key:\n",
    "                    del self.table[key_hash][index]\n",
    "                    return True\n",
    "\n",
    "    def all(self):\n",
    "        print(self.table)\n",
    "\n",
    "\n",
    "# Testing\n",
    "H = HashTable(5)\n",
    "H.insert(\"apple\", 10)\n",
    "H.insert(\"orange\", 20)\n",
    "H.insert(\"banana\", 30)\n",
    "\n",
    "H.all()  # [[], [['orange', 20]], [], [], [['apple', 10], ['banana', 30]]]\n",
    "\n",
    "H.delete(\"orange\")\n",
    "print(H.get(\"orange\"))  # return: None\n",
    "\n",
    "H.delete(\"banana\")\n",
    "print(H.get(\"banana\"))  # return: None\n",
    "\n",
    "H.delete(\"blabla\")\n",
    "print(H.get(\"blabla\"))  # return: None\n",
    "\n",
    "H.all()  # [[], [], [], [], [['apple', 10]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a binary search for a sorted array with fractional numbers. A function written for binary search should return a tuple where the first element is the number of iterations required to find the element. The second element should be the \"upper limit\" - this is the smallest element that is greater than or equal to the given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 4\n",
      "Upper limit: 1.5\n"
     ]
    }
   ],
   "source": [
    "def binary_search(array: list, target: float) -> tuple[int, float]:\n",
    "    left_index = 0\n",
    "    right_index = len(array) - 1\n",
    "    iterations = 0\n",
    "\n",
    "    while left_index <= right_index:\n",
    "        mid_index = left_index + (right_index - left_index) // 2\n",
    "        iterations += 1\n",
    "\n",
    "        if target == array[mid_index]:\n",
    "            return iterations, array[mid_index]\n",
    "\n",
    "        elif target > array[mid_index]:\n",
    "            left_index = mid_index + 1\n",
    "\n",
    "        else:\n",
    "            right_index = mid_index - 1\n",
    "\n",
    "    # If the exact target is not present in the array\n",
    "    if left_index < len(array):  # The nearest larger value\n",
    "        upper_limit = array[left_index]\n",
    "    else:  # If target is larger than max element of the array\n",
    "        upper_limit = None\n",
    "\n",
    "    return iterations, upper_limit\n",
    "\n",
    "\n",
    "# Testing\n",
    "arr = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5]\n",
    "target = 1.4\n",
    "iterations, upper_limit = binary_search(arr, target)\n",
    "print(\"Iterations:\", iterations)\n",
    "print(\"Upper limit:\", upper_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of Boyer-Moore, Knuth-Morris-Pratt and Rabin-Karp substring search algorithms based on two text files ([article 1](https://drive.google.com/file/d/18_R5vEQ3eDuy2VdV3K5Lu-R-B-adxXZh/view), [article 2](https://drive.google.com/file/d/13hSt4JkJc11nckZZz2yoFHYL89a4XkMZ/view)). Using timeit, it is necessary to measure the execution time of each algorithm for two types of substrings: one that actually exists in the text, and the other - a fictional one (the choice of substrings is your choice). Based on the received data, determine the fastest algorithm for each text separately and as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Boyer-Moore algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shift_table(pattern: str) -> dict:\n",
    "    \"\"\"\n",
    "    Create a shift table for the Boyer-Moore algorithm\n",
    "    \"\"\"\n",
    "    table = {}\n",
    "    length = len(pattern)\n",
    "    # For each character in the substring, set an offset equal to the length of the substring\n",
    "    for index, char in enumerate(pattern[:-1]):\n",
    "        table[char] = length - index - 1\n",
    "    # If the character is not in the table, the offset will be equal to the length of the substring\n",
    "    table.setdefault(pattern[-1], length)\n",
    "    return table\n",
    "\n",
    "\n",
    "def boyer_moore_search(main_string: str, pattern: str) -> int:\n",
    "    # Create a table of shifts for the pattern (substring)\n",
    "    shift_table = build_shift_table(pattern)\n",
    "    i = 0  # Initialize the initial index for the main text\n",
    "\n",
    "    # Go through the main text, comparing with the substring\n",
    "    while i <= len(main_string) - len(pattern):\n",
    "        j = len(pattern) - 1  # Start from the end of the substring\n",
    "\n",
    "        # Compare characters from the end of the substring to its beginning\n",
    "        while j >= 0 and main_string[i + j] == pattern[j]:\n",
    "            j -= 1  # Move to the beginning of the substring\n",
    "\n",
    "        # If the entire substring matches, return its position in the text\n",
    "        if j < 0:\n",
    "            return i\n",
    "\n",
    "        # Shift the index i based on the shift table\n",
    "        # This allows to \"jump\" over non-matching parts of the text\n",
    "        i += shift_table.get(main_string[i + len(pattern) - 1], len(pattern))\n",
    "\n",
    "    # If the substring is not found, return -1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Knuth-Morris-Pratt algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lps(pattern: str) -> list:\n",
    "    \"\"\"\n",
    "    Computes lps list (longest prefix suffixes)\n",
    "    \"\"\"\n",
    "    lps = [0] * len(pattern)\n",
    "    length = 0\n",
    "    i = 1\n",
    "\n",
    "    while i < len(pattern):\n",
    "        if pattern[i] == pattern[length]:\n",
    "            length += 1\n",
    "            lps[i] = length\n",
    "            i += 1\n",
    "        else:\n",
    "            if length != 0:\n",
    "                length = lps[length - 1]\n",
    "            else:\n",
    "                lps[i] = 0\n",
    "                i += 1\n",
    "\n",
    "    return lps\n",
    "\n",
    "\n",
    "def kmp_search(main_string: str, pattern: str) -> int:\n",
    "    M = len(pattern)\n",
    "    N = len(main_string)\n",
    "\n",
    "    lps = compute_lps(pattern)\n",
    "\n",
    "    i = j = 0\n",
    "\n",
    "    while i < N:\n",
    "        if pattern[j] == main_string[i]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif j != 0:\n",
    "            j = lps[j - 1]\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "        if j == M:\n",
    "            return i - j\n",
    "\n",
    "    return -1  # If the substring is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Rabin-Karp algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_hash(string: str, base: int = 256, modulus: int = 101) -> int:\n",
    "    \"\"\"\n",
    "    Returns the polynomial hash of string\n",
    "    \"\"\"\n",
    "    n = len(string)\n",
    "    hash_value = 0\n",
    "    for i, char in enumerate(string):\n",
    "        power_of_base = pow(base, n - i - 1) % modulus\n",
    "        hash_value = (hash_value + ord(char) * power_of_base) % modulus\n",
    "    return hash_value\n",
    "\n",
    "\n",
    "def rabin_karp_search(main_string: str, pattern: str) -> int:\n",
    "    # Main string and search substring lengths\n",
    "    substring_length = len(pattern)\n",
    "    main_string_length = len(main_string)\n",
    "\n",
    "    # Hash base number and modulus\n",
    "    base = 256\n",
    "    modulus = 101\n",
    "\n",
    "    # The hash value for the search substring and the current segment in the main string\n",
    "    substring_hash = polynomial_hash(pattern, base, modulus)\n",
    "    current_slice_hash = polynomial_hash(main_string[:substring_length], base, modulus)\n",
    "\n",
    "    # Default value for hash recalculation\n",
    "    h_multiplier = pow(base, substring_length - 1) % modulus\n",
    "\n",
    "    # Run through the main line\n",
    "    for i in range(main_string_length - substring_length + 1):\n",
    "        if substring_hash == current_slice_hash:\n",
    "            if main_string[i : i + substring_length] == pattern:\n",
    "                return i\n",
    "\n",
    "        if i < main_string_length - substring_length:\n",
    "            current_slice_hash = (\n",
    "                current_slice_hash - ord(main_string[i]) * h_multiplier\n",
    "            ) % modulus\n",
    "            current_slice_hash = (\n",
    "                current_slice_hash * base + ord(main_string[i + substring_length])\n",
    "            ) % modulus\n",
    "            if current_slice_hash < 0:\n",
    "                current_slice_hash += modulus\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import requests\n",
    "\n",
    "url_1 = (\n",
    "    \"https://drive.google.com/file/d/18_R5vEQ3eDuy2VdV3K5Lu-R-B-adxXZh/view?usp=sharing\"\n",
    ")\n",
    "url_2 = (\n",
    "    \"https://drive.google.com/file/d/13hSt4JkJc11nckZZz2yoFHYL89a4XkMZ/view?usp=sharing\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_txt_file(url: str) -> str:\n",
    "    \"\"\"\n",
    "    The function of extracting text from files on Google Drive\n",
    "\n",
    "    :param url: link to the file\n",
    "    :return: extracted and decoded text\n",
    "    \"\"\"\n",
    "    file_id = url.split(\"/\")[-2]\n",
    "    request_url = \"https://drive.google.com/uc?id=\" + file_id\n",
    "    response = requests.get(request_url)\n",
    "    raw_data = response.content\n",
    "    encoding = chardet.detect(raw_data)[\"encoding\"]\n",
    "    return raw_data.decode(encoding)\n",
    "\n",
    "\n",
    "file_1 = get_txt_file(url_1)\n",
    "file_2 = get_txt_file(url_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 - Patterns preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бібліотеках мов програмування', 'експоненціальний пошук використовується', 'потрібно обрати чергові', 'щоб їх було', 'досягнення локального мінімуму', 'ykurcinrjl', 'apdjcbvtfe', 'rneazqiawq', 'dvwzxzgkah', 'zrxlgxjbkd']\n",
      "['на цьому етапі', 'послідовному розташуванню елементів', 'зору якості її', 'є можливість відфільтрувати', 'здійснюється пошук усіх', 'tabymbtgub', 'dramqhwexd', 'kxoywydbrh', 'oouxjsjamo', 'cysvqjbnrf']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def prepare_patterns(text: str) -> list:\n",
    "    \"\"\"\n",
    "    A function that randomly selects 5 strings from the text and\n",
    "    creates 5 random strings that are not in the text\n",
    "\n",
    "    :param text: the text from which 5 random strings are extracted,\n",
    "    each string contains 3 words\n",
    "    :retun: a list with 5 existing and 5 non-existent strings in\n",
    "    the text\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Preparation of existing patterns\n",
    "    substrings = re.findall(r\"\\b\\w+\\b \\b\\w+\\b \\b\\w+\\b\", text)\n",
    "    samples = random.sample(substrings, 5)\n",
    "    for sample in samples:\n",
    "        results.append(sample.lower())\n",
    "\n",
    "    # Preparation of non-existent patterns\n",
    "    characters = string.ascii_letters\n",
    "    for i in range(5):\n",
    "        random_string = \"\".join(random.choice(characters) for _ in range(10))\n",
    "        results.append(random_string.lower())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "test_patterns_1 = prepare_patterns(file_1)\n",
    "test_patterns_2 = prepare_patterns(file_2)\n",
    "\n",
    "print(test_patterns_1)\n",
    "print(test_patterns_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 - Testing algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "def test_algorithms(algorithms: dict, main_string: str, patterns: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function for testing searching algorithms on a text\n",
    "\n",
    "    :param algorithms: a dictionary, where the keys are algorithm names, and\n",
    "    the values are functions or methods with an implemented searching algorithm\n",
    "    :param main_string: text where the search for patterns will be carried out\n",
    "    :param paterns: a list of strings to search for in the text\n",
    "    :return: a dictionary with algorithm names as keys and times spent for\n",
    "    searching as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for algo_name, algo_func in algorithms.items():\n",
    "        results[algo_name] = 0\n",
    "        for pattern in patterns:\n",
    "            # time_taken = timeit.timeit(lambda: algo_func(main_string, pattern), number=10)\n",
    "            time_taken = measure_search_time(algo_func, main_string, pattern)\n",
    "            results[algo_name] += time_taken\n",
    "    return results\n",
    "\n",
    "\n",
    "def measure_search_time(func, main_string: str, pattern: str) -> int:\n",
    "    \"\"\"\n",
    "    The function of calculating the time required to search with a specific\n",
    "    functionfor a pattern in the text\n",
    "\n",
    "    :param function: a function with an implemented searching algorithm\n",
    "    :param main_string: text where the search for patterns will be carried out\n",
    "    :param patern: a strings to search for in the text\n",
    "    :return: time of search\n",
    "    \"\"\"\n",
    "    setup_code = f\"\"\"from __main__ import {func.__name__}\"\"\"\n",
    "    stmt = f\"{func.__name__}(main_string, pattern)\"\n",
    "    return timeit.timeit(\n",
    "        stmt,\n",
    "        setup=setup_code,\n",
    "        globals={\"main_string\": main_string, \"pattern\": pattern},\n",
    "        number=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    \"Boyer-Moore\": boyer_moore_search,\n",
    "    \"Knuth-Morris-Pratt\": kmp_search,\n",
    "    \"Rabin-Karp\": rabin_karp_search,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the first file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: Boyer-Moore          Time taken: 0.032971 seconds\n",
      "Algorithm: Knuth-Morris-Pratt   Time taken: 0.117601 seconds\n",
      "Algorithm: Rabin-Karp           Time taken: 0.288353 seconds\n"
     ]
    }
   ],
   "source": [
    "results_1 = test_algorithms(algorithms, file_1.lower(), test_patterns_1)\n",
    "for algo, timings in results_1.items():\n",
    "    print(f\"Algorithm: {algo:<20} Time taken: {timings:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the second file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: Boyer-Moore          Time taken: 0.046305 seconds\n",
      "Algorithm: Knuth-Morris-Pratt   Time taken: 0.146952 seconds\n",
      "Algorithm: Rabin-Karp           Time taken: 0.408292 seconds\n"
     ]
    }
   ],
   "source": [
    "results_2 = test_algorithms(algorithms, file_2.lower(), test_patterns_2)\n",
    "for algo, timings in results_2.items():\n",
    "    print(f\"Algorithm: {algo:<20} Time taken: {timings:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "As can be seen from the testing of both text files, the Boyer-Moore algorithm is significantly faster than all the others. This can be explained by the fact that it has a more efficient shifting strategy by searching the substring from right to left and a special shifting table that shows how much to shift the substring if the characters in the text search do not match. In some circumstances, when no substring characters match the text characters, forcing the algorithm to make the maximum possible shift on each failed match attempt, the Boyer-Moore algorithm can even achieve linear complexity `O(n)`, where `n` is the length of the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
